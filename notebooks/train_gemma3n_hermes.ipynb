{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Gemma3N with Hermes Datasets\n",
    "\n",
    "This notebook trains a custom Gemma3N model with:\n",
    "- Uncensored behavior (removed safety filters)\n",
    "- Enhanced function calling capabilities\n",
    "- Multimodal support (text, audio, image)\n",
    "- Based on Hermes-3-Dataset and hermes-function-calling-v1\n",
    "\n",
    "**Output:** GGUF Q8_0 quantized model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "import os, re\n",
    "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
    "    !pip install unsloth\n",
    "else:\n",
    "    # Do this only in Colab notebooks! Otherwise use pip install unsloth\n",
    "    import torch; v = re.match(r\"[0-9\\.]{3,}\", str(torch.__version__)).group(0)\n",
    "    xformers = \"xformers==\" + (\"0.0.32.post2\" if v == \"2.8.0\" else \"0.0.29.post3\")\n",
    "    !pip install --no-deps bitsandbytes accelerate {xformers} peft trl triton cut_cross_entropy unsloth_zoo\n",
    "    !pip install sentencepiece protobuf \"datasets>=3.4.1,<4.0.0\" \"huggingface_hub>=0.34.0\" hf_transfer\n",
    "    !pip install --no-deps unsloth\n",
    "!pip install transformers==4.55.4\n",
    "import torch; torch._dynamo.config.recompile_limit = 64;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install --no-deps --upgrade timm # Only for Gemma 3N"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastModel\n",
    "import torch\n",
    "\n",
    "model, tokenizer = FastModel.from_pretrained(\n",
    "    model_name = \"unsloth/gemma-3n-E4B-it\",\n",
    "    dtype = None, # None for auto detection\n",
    "    max_seq_length = 2048, # Longer context for complex tasks\n",
    "    load_in_4bit = True,  # 4 bit quantization to reduce memory\n",
    "    full_finetuning = False, # Use LoRA for efficiency\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LoRA Configuration for Uncensored Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FastModel.get_peft_model(\n",
    "    model,\n",
    "    finetune_vision_layers     = True, # Enable vision fine-tuning\n",
    "    finetune_language_layers   = True,  # Language layers for uncensored training\n",
    "    finetune_attention_modules = True,  # Attention for function calling\n",
    "    finetune_mlp_modules       = True,  # MLPs for complex reasoning\n",
    "\n",
    "    r = 16,           # Higher rank for better capacity\n",
    "    lora_alpha = 16,  # Recommended alpha == r\n",
    "    lora_dropout = 0.05, # Small dropout for regularization\n",
    "    bias = \"none\",\n",
    "    random_state = 3407,\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "\n",
    "Combine Hermes-3-Dataset and hermes-function-calling-v1 for:\n",
    "- Uncensored conversations\n",
    "- Function calling examples\n",
    "- Multimodal training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, concatenate_datasets\n",
    "from unsloth.chat_templates import get_chat_template, standardize_data_formats\n",
    "import json\n",
    "\n",
    "# Load Hermes datasets\n",
    "print(\"Loading Hermes-3-Dataset...\")\n",
    "hermes3_dataset = load_dataset(\"NousResearch/Hermes-3-Dataset\", split=\"train\")\n",
    "\n",
    "print(\"Loading hermes-function-calling-v1...\")\n",
    "hermes_fc_dataset = load_dataset(\"NousResearch/hermes-function-calling-v1\", split=\"train\")\n",
    "\n",
    "# Combine datasets\n",
    "print(\"Combining datasets...\")\n",
    "combined_dataset = concatenate_datasets([hermes3_dataset, hermes_fc_dataset])\n",
    "\n",
    "# Shuffle and limit for training efficiency\n",
    "combined_dataset = combined_dataset.shuffle(seed=42)\n",
    "dataset = combined_dataset.select(range(min(10000, len(combined_dataset))))  # Use up to 10k samples\n",
    "\n",
    "print(f\"Training on {len(dataset)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply chat template\n",
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template = \"gemma-3\",\n",
    ")\n",
    "\n",
    "# Standardize data format\n",
    "dataset = standardize_data_formats(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format conversations for training\n",
    "def formatting_prompts_func(examples):\n",
    "    convos = examples[\"conversations\"]\n",
    "    texts = [tokenizer.apply_chat_template(convo, tokenize = False, add_generation_prompt = False).removeprefix('<bos>') for convo in convos]\n",
    "    return { \"text\" : texts, }\n",
    "\n",
    "dataset = dataset.map(formatting_prompts_func, batched = True)\n",
    "\n",
    "# Verify format\n",
    "print(\"Sample formatted text:\")\n",
    "print(dataset[0][\"text\"][:500] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Configuration\n",
    "\n",
    "Configure for uncensored, function-calling capable model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTTrainer, SFTConfig\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = dataset,\n",
    "    eval_dataset = None,\n",
    "    args = SFTConfig(\n",
    "        dataset_text_field = \"text\",\n",
    "        per_device_train_batch_size = 2,\n",
    "        gradient_accumulation_steps = 4,\n",
    "        warmup_steps = 10,\n",
    "        num_train_epochs = 3,  # Full training run\n",
    "        max_steps = None,  # Use epochs instead\n",
    "        learning_rate = 2e-4,\n",
    "        logging_steps = 10,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"cosine\",\n",
    "        seed = 3407,\n",
    "        report_to = \"none\",\n",
    "        save_steps = 500,\n",
    "        save_total_limit = 3,\n",
    "        fp16 = True,\n",
    "        gradient_checkpointing = True,\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train on responses only for better efficiency\n",
    "from unsloth.chat_templates import train_on_responses_only\n",
    "trainer = train_on_responses_only(\n",
    "    trainer,\n",
    "    instruction_part = \"<start_of_turn>user\\n\",\n",
    "    response_part = \"<start_of_turn>model\\n\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show memory stats\n",
    "gpu_stats = torch.cuda.get_device_properties(0)\n",
    "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
    "print(f\"{start_gpu_memory} GB of memory reserved.\")\n",
    "\n",
    "# Train!\n",
    "trainer_stats = trainer.train()\n",
    "\n",
    "# Show final stats\n",
    "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
    "used_percentage = round(used_memory / max_memory * 100, 3)\n",
    "lora_percentage = round(used_memory_for_lora / max_memory * 100, 3)\n",
    "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
    "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
    "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save LoRA adapters\n",
    "model.save_pretrained(\"gemma3n-hermes-lora\")\n",
    "tokenizer.save_pretrained(\"gemma3n-hermes-lora\")\n",
    "\n",
    "# Merge and save full model\n",
    "model = model.merge_and_unload()\n",
    "model.save_pretrained(\"gemma3n-hermes-merged\")\n",
    "tokenizer.save_pretrained(\"gemma3n-hermes-merged\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert to GGUF Q8_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install llama.cpp for conversion\n",
    "!git clone https://github.com/ggerganov/llama.cpp\n",
    "%cd llama.cpp\n",
    "!make\n",
    "\n",
    "# Convert to GGUF\n",
    "!python convert_hf_to_gguf.py ../gemma3n-hermes-merged \\\n",
    "    --outtype q8_0 \\\n",
    "    --outfile ../gemma3n-hermes-Q8_0.gguf\n",
    "\n",
    "print(\"GGUF conversion complete!\")\n",
    "print(\"Model saved as: gemma3n-hermes-Q8_0.gguf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test function calling\n",
    "from transformers import TextStreamer\n",
    "\n",
    "def test_inference(messages, max_new_tokens=256):\n",
    "    inputs = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        add_generation_prompt=True,\n",
    "        return_tensors=\"pt\",\n",
    "        tokenize=True,\n",
    "        return_dict=True,\n",
    "    ).to(\"cuda\")\n",
    "    \n",
    "    _ = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        temperature=0.8,  # Slightly lower for more coherent responses\n",
    "        top_p=0.9,\n",
    "        top_k=50,\n",
    "        streamer=TextStreamer(tokenizer, skip_prompt=True),\n",
    "    )\n",
    "\n",
    "# Test function calling capability\n",
    "messages = [{\n",
    "    \"role\": \"user\",\n",
    "    \"content\": \"Write a Python function to calculate fibonacci numbers and call it with n=10\"\n",
    "}]\n",
    "\n",
    "print(\"Testing function calling:\")\n",
    "test_inference(messages)\n",
    "\n",
    "# Test multimodal (if you have test image/audio)\n",
    "# messages = [{\n",
    "#     \"role\": \"user\",\n",
    "#     \"content\": [\n",
    "#         {\"type\": \"text\", \"text\": \"What's in this image?\"},\n",
    "#         {\"type\": \"image\", \"image\": \"path/to/test/image.jpg\"}\n",
    "#     ]\n",
    "# }]\n",
    "# test_inference(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Specifications\n",
    "\n",
    "- **Base Model:** Gemma3N 4B Instruction Tuned\n",
    "- **Training Data:** Hermes-3-Dataset + hermes-function-calling-v1\n",
    "- **Capabilities:** \n",
    "  - Uncensored responses\n",
    "  - Function calling\n",
    "  - Multimodal (text, audio, image)\n",
    "- **Quantization:** GGUF Q8_0\n",
    "- **Use Cases:** General AI assistant with advanced capabilities"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
